{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "399b3e40",
   "metadata": {},
   "source": [
    "# Extract Detailed Species Information from Vietnamese Red List\n",
    "\n",
    "This notebook fetches detailed information for each species from their individual pages on the Vietnamese Red List website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c217bcc4",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6a30843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, Optional, List\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079a5a48",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "110cc1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headers to mimic a browser request\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Input file containing species links\n",
    "INPUT_CSV = 'vnredlist_all_species_links.csv'\n",
    "\n",
    "# Output file for detailed species data\n",
    "OUTPUT_JSON = 'vnredlist_species_details.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3db238a",
   "metadata": {},
   "source": [
    "## 3. Helper Functions to Extract Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5d6ca24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_species_page(url: str) -> Optional[BeautifulSoup]:\n",
    "    \"\"\"\n",
    "    Fetch a species page and return BeautifulSoup object.\n",
    "    \n",
    "    Args:\n",
    "        url: The species page URL\n",
    "    \n",
    "    Returns:\n",
    "        BeautifulSoup object or None if failed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        return BeautifulSoup(response.content, 'html.parser')\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"âœ— Error fetching {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c220b06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_after_heading(soup: BeautifulSoup, heading_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract text content after a specific heading.\n",
    "    \n",
    "    Args:\n",
    "        soup: BeautifulSoup object\n",
    "        heading_text: The heading text to search for\n",
    "    \n",
    "    Returns:\n",
    "        Extracted text or empty string\n",
    "    \"\"\"\n",
    "    # Find the heading (h3, h4, etc.)\n",
    "    headings = soup.find_all(['h3', 'h4'])\n",
    "    \n",
    "    for heading in headings:\n",
    "        heading_content = heading.get_text(strip=True)\n",
    "        if heading_text.lower() in heading_content.lower():\n",
    "            # Get the next sibling element\n",
    "            next_elem = heading.find_next_sibling()\n",
    "            if next_elem:\n",
    "                # Get text with proper spacing\n",
    "                text = next_elem.get_text(separator=' ', strip=True)\n",
    "                # Clean up multiple spaces\n",
    "                text = ' '.join(text.split())\n",
    "                return text\n",
    "    \n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9526976b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_page_structure(soup: BeautifulSoup):\n",
    "    \"\"\"\n",
    "    Debug function to inspect the page structure.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Page Structure Analysis ===\")\n",
    "    \n",
    "    # Find all h3 and h4 headings\n",
    "    print(\"\\nAll H3/H4 headings found:\")\n",
    "    for heading in soup.find_all(['h3', 'h4'])[:20]:  # Limit to first 20\n",
    "        text = heading.get_text(strip=True)\n",
    "        next_elem = heading.find_next_sibling()\n",
    "        next_text = next_elem.get_text(strip=True)[:100] if next_elem else \"None\"\n",
    "        print(f\"  â€¢ {text}\")\n",
    "        print(f\"    â†’ {next_text}\")\n",
    "    \n",
    "    print(\"\\n=== End of Structure Analysis ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30f2aec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_taxonomic_info(soup: BeautifulSoup) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Extract taxonomic classification information.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with taxonomic ranks\n",
    "    \"\"\"\n",
    "    taxonomy = {\n",
    "        'kingdom_latin': '',\n",
    "        'kingdom_vi': '',\n",
    "        'phylum_latin': '',\n",
    "        'phylum_vi': '',\n",
    "        'class_latin': '',\n",
    "        'class_vi': '',\n",
    "        'order_latin': '',\n",
    "        'order_vi': '',\n",
    "        'family_latin': '',\n",
    "        'family_vi': ''\n",
    "    }\n",
    "    \n",
    "    # Map Vietnamese headings to taxonomy keys (exact matches)\n",
    "    taxonomy_map = {\n",
    "        'Giá»›i': 'kingdom',\n",
    "        'NgÃ nh': 'phylum',\n",
    "        'Lá»›p': 'class',\n",
    "        'Bá»™': 'order',\n",
    "        'Há»': 'family'\n",
    "    }\n",
    "    \n",
    "    # Find the taxonomy section (usually under \"PhÃ¢n loáº¡i\")\n",
    "    headings = soup.find_all(['h3', 'h4'])\n",
    "    \n",
    "    for heading in headings:\n",
    "        heading_text = heading.get_text(strip=True)\n",
    "        \n",
    "        # Check if this heading matches one of our taxonomy levels\n",
    "        for vn_name, en_key in taxonomy_map.items():\n",
    "            if heading_text == vn_name:  # Exact match only\n",
    "                # Get the next sibling element\n",
    "                next_elem = heading.find_next_sibling()\n",
    "                if next_elem:\n",
    "                    value = next_elem.get_text(separator=' ', strip=True)\n",
    "                    value = ' '.join(value.split())  # Clean multiple spaces\n",
    "                    taxonomy[f'{en_key}_latin'] = value.upper()\n",
    "                break\n",
    "    \n",
    "    return taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "769741ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_species_details(url: str, soup: BeautifulSoup) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract all species details from a species page.\n",
    "    \n",
    "    Args:\n",
    "        url: The species page URL\n",
    "        soup: BeautifulSoup object of the page\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with species information\n",
    "    \"\"\"\n",
    "    species_data = {\n",
    "        'scientific_name': {'value': '', 'note': ''},\n",
    "        'common_name': {'value': '', 'note': ''},\n",
    "        'kingdom_latin': '',\n",
    "        'kingdom_vi': '',\n",
    "        'phylum_latin': '',\n",
    "        'phylum_vi': '',\n",
    "        'class_latin': '',\n",
    "        'class_vi': '',\n",
    "        'order_latin': '',\n",
    "        'order_vi': '',\n",
    "        'family_latin': '',\n",
    "        'family_vi': '',\n",
    "        'note': '',\n",
    "        'laws': []\n",
    "    }\n",
    "    \n",
    "    # Extract scientific name (from page title or heading)\n",
    "    title = soup.find('h1')\n",
    "    if title:\n",
    "        sci_name = title.get_text(strip=True)\n",
    "        # Clean scientific name - remove author and year if present in title\n",
    "        # Keep only the first 2-3 words (genus, species, and possibly subspecies)\n",
    "        parts = sci_name.split()\n",
    "        if len(parts) >= 2:\n",
    "            # Check if third word starts with lowercase (subspecies) or uppercase (author)\n",
    "            if len(parts) >= 3 and parts[2][0].islower():\n",
    "                sci_name = ' '.join(parts[:3])\n",
    "            else:\n",
    "                sci_name = ' '.join(parts[:2])\n",
    "        species_data['scientific_name']['value'] = sci_name\n",
    "    \n",
    "    # Extract Vietnamese common name\n",
    "    common_name = extract_text_after_heading(soup, 'TÃªn viá»‡t nam')\n",
    "    if common_name:\n",
    "        species_data['common_name']['value'] = common_name\n",
    "    \n",
    "    # Extract taxonomic information\n",
    "    taxonomy = extract_taxonomic_info(soup)\n",
    "    species_data.update(taxonomy)\n",
    "    \n",
    "    # Extract conservation status from Vietnamese Red List\n",
    "    # Try multiple possible headings\n",
    "    status = extract_text_after_heading(soup, 'PhÃ¢n háº¡ng báº£o tá»“n')\n",
    "    if not status:\n",
    "        status = extract_text_after_heading(soup, 'PhÃ¢n háº¡ng')\n",
    "    \n",
    "    # Extract just the status code (CR, EN, VU, etc.)\n",
    "    if status:\n",
    "        # Look for status codes in the text\n",
    "        status_match = re.search(r'\\b(CR|EN|VU|NT|LC|DD|EW|EX|NE)\\b', status)\n",
    "        if status_match:\n",
    "            status_code = status_match.group(1)\n",
    "            species_data['laws'].append({\n",
    "                'name': {\n",
    "                    'vi': 'Danh lá»¥c Äá» Viá»‡t Nam',\n",
    "                    'en': 'Vietnam Red List'\n",
    "                },\n",
    "                'value': status_code,\n",
    "                'note': url\n",
    "            })\n",
    "    \n",
    "    return species_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40783787",
   "metadata": {},
   "source": [
    "## 4. Load Species Links from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63600ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded 1357 species links from vnredlist_all_species_links.csv\n",
      "Columns: ['scientific_name', 'common_name_vi', 'url', 'category', 'category_url']\n",
      "\n",
      "First few rows:\n",
      "                scientific_name  common_name_vi  \\\n",
      "0               Elephas maximus      Voi chÃ¢u Ã¡   \n",
      "1        Galeopterus variegatus        Chá»“n dÆ¡i   \n",
      "2        Nycticebus bengalensis       Cu li lá»›n   \n",
      "3  Xanthonycticebus intermedius  Cu li miá»n báº¯c   \n",
      "4     Xanthonycticebus pygmaeus       Cu li nhá»   \n",
      "\n",
      "                                                 url  \\\n",
      "0          http://vnredlist.vast.vn/elephas-maximus/   \n",
      "1   http://vnredlist.vast.vn/galeopterus-variegatus/   \n",
      "2   http://vnredlist.vast.vn/nycticebus-bengalensis/   \n",
      "3  http://vnredlist.vast.vn/xanthonycticebus-inte...   \n",
      "4  http://vnredlist.vast.vn/xanthonycticebus-pygm...   \n",
      "\n",
      "                       category  \\\n",
      "0  dong-vat-co-day-song/lop-thu   \n",
      "1  dong-vat-co-day-song/lop-thu   \n",
      "2  dong-vat-co-day-song/lop-thu   \n",
      "3  dong-vat-co-day-song/lop-thu   \n",
      "4  dong-vat-co-day-song/lop-thu   \n",
      "\n",
      "                                        category_url  \n",
      "0  http://vnredlist.vast.vn/dong-vat/dong-vat-co-...  \n",
      "1  http://vnredlist.vast.vn/dong-vat/dong-vat-co-...  \n",
      "2  http://vnredlist.vast.vn/dong-vat/dong-vat-co-...  \n",
      "3  http://vnredlist.vast.vn/dong-vat/dong-vat-co-...  \n",
      "4  http://vnredlist.vast.vn/dong-vat/dong-vat-co-...  \n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file with species links\n",
    "try:\n",
    "    df_species = pd.read_csv(INPUT_CSV)\n",
    "    print(f\"âœ“ Loaded {len(df_species)} species links from {INPUT_CSV}\")\n",
    "    print(f\"Columns: {df_species.columns.tolist()}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(df_species.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"âœ— File {INPUT_CSV} not found. Please run the fetch_animal_links.ipynb notebook first.\")\n",
    "    df_species = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bc9cb7",
   "metadata": {},
   "source": [
    "## 5. Test with a Single Species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be1761f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with: http://vnredlist.vast.vn/xanthonycticebus-intermedius/\n",
      "================================================================================\n",
      "\n",
      "Extracted data:\n",
      "{\n",
      "  \"scientific_name\": {\n",
      "    \"value\": \"Xanthonycticebus intermedius\",\n",
      "    \"note\": \"\"\n",
      "  },\n",
      "  \"common_name\": {\n",
      "    \"value\": \"Cu li miá»n báº¯c\",\n",
      "    \"note\": \"\"\n",
      "  },\n",
      "  \"kingdom_latin\": \"ANIMALIA\",\n",
      "  \"kingdom_vi\": \"\",\n",
      "  \"phylum_latin\": \"CHORDATA\",\n",
      "  \"phylum_vi\": \"\",\n",
      "  \"class_latin\": \"MAMMALIA\",\n",
      "  \"class_vi\": \"\",\n",
      "  \"order_latin\": \"PRIMATES\",\n",
      "  \"order_vi\": \"\",\n",
      "  \"family_latin\": \"LORISIDAE\",\n",
      "  \"family_vi\": \"\",\n",
      "  \"note\": \"\",\n",
      "  \"laws\": [\n",
      "    {\n",
      "      \"name\": {\n",
      "        \"vi\": \"Danh lá»¥c Äá» Viá»‡t Nam\",\n",
      "        \"en\": \"Vietnam Red List\"\n",
      "      },\n",
      "      \"value\": \"EN\",\n",
      "      \"note\": \"http://vnredlist.vast.vn/xanthonycticebus-intermedius/\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "Verification:\n",
      "âœ“ Scientific name: Xanthonycticebus intermedius\n",
      "âœ“ Common name: Cu li miá»n báº¯c\n",
      "âœ“ Kingdom: ANIMALIA\n",
      "âœ“ Phylum: CHORDATA\n",
      "âœ“ Class: MAMMALIA\n",
      "âœ“ Order: PRIMATES\n",
      "âœ“ Family: LORISIDAE\n",
      "âœ“ Conservation status: EN\n",
      "================================================================================\n",
      "\n",
      "Extracted data:\n",
      "{\n",
      "  \"scientific_name\": {\n",
      "    \"value\": \"Xanthonycticebus intermedius\",\n",
      "    \"note\": \"\"\n",
      "  },\n",
      "  \"common_name\": {\n",
      "    \"value\": \"Cu li miá»n báº¯c\",\n",
      "    \"note\": \"\"\n",
      "  },\n",
      "  \"kingdom_latin\": \"ANIMALIA\",\n",
      "  \"kingdom_vi\": \"\",\n",
      "  \"phylum_latin\": \"CHORDATA\",\n",
      "  \"phylum_vi\": \"\",\n",
      "  \"class_latin\": \"MAMMALIA\",\n",
      "  \"class_vi\": \"\",\n",
      "  \"order_latin\": \"PRIMATES\",\n",
      "  \"order_vi\": \"\",\n",
      "  \"family_latin\": \"LORISIDAE\",\n",
      "  \"family_vi\": \"\",\n",
      "  \"note\": \"\",\n",
      "  \"laws\": [\n",
      "    {\n",
      "      \"name\": {\n",
      "        \"vi\": \"Danh lá»¥c Äá» Viá»‡t Nam\",\n",
      "        \"en\": \"Vietnam Red List\"\n",
      "      },\n",
      "      \"value\": \"EN\",\n",
      "      \"note\": \"http://vnredlist.vast.vn/xanthonycticebus-intermedius/\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "Verification:\n",
      "âœ“ Scientific name: Xanthonycticebus intermedius\n",
      "âœ“ Common name: Cu li miá»n báº¯c\n",
      "âœ“ Kingdom: ANIMALIA\n",
      "âœ“ Phylum: CHORDATA\n",
      "âœ“ Class: MAMMALIA\n",
      "âœ“ Order: PRIMATES\n",
      "âœ“ Family: LORISIDAE\n",
      "âœ“ Conservation status: EN\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test with the first species\n",
    "if not df_species.empty:\n",
    "    test_url = df_species.iloc[3]['url']\n",
    "    print(f\"Testing with: {test_url}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    test_soup = fetch_species_page(test_url)\n",
    "    if test_soup:\n",
    "        # Extract data\n",
    "        test_data = extract_species_details(test_url, test_soup)\n",
    "        print(\"\\nExtracted data:\")\n",
    "        print(json.dumps(test_data, indent=2, ensure_ascii=False))\n",
    "        \n",
    "        # Verify the extraction\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"Verification:\")\n",
    "        print(f\"âœ“ Scientific name: {test_data['scientific_name']['value']}\")\n",
    "        print(f\"âœ“ Common name: {test_data['common_name']['value']}\")\n",
    "        print(f\"âœ“ Kingdom: {test_data['kingdom_latin']}\")\n",
    "        print(f\"âœ“ Phylum: {test_data['phylum_latin']}\")\n",
    "        print(f\"âœ“ Class: {test_data['class_latin']}\")\n",
    "        print(f\"âœ“ Order: {test_data['order_latin']}\")\n",
    "        print(f\"âœ“ Family: {test_data['family_latin']}\")\n",
    "        if test_data['laws']:\n",
    "            print(f\"âœ“ Conservation status: {test_data['laws'][0]['value']}\")\n",
    "        print(\"=\" * 80)\n",
    "    else:\n",
    "        print(\"Failed to fetch test page\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6482d7",
   "metadata": {},
   "source": [
    "## 6. Process All Species\n",
    "\n",
    "Extract detailed information for all species with proper delays between requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ca04d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to process 1357 species...\n",
      "================================================================================\n",
      "\n",
      "[1/1357] Processing: Elephas maximus\n",
      "\n",
      "[10/1357] Processing: Macaca mulatta\n",
      "  âœ“ Successfully extracted data\n",
      "\n",
      "  ðŸ’¾ Saved intermediate results to vnredlist_species_details_partial_10.json\n",
      "\n",
      "[20/1357] Processing: Trachypithecus margarita\n",
      "  âœ“ Successfully extracted data\n",
      "\n",
      "  ðŸ’¾ Saved intermediate results to vnredlist_species_details_partial_20.json\n",
      "\n",
      "[30/1357] Processing: Neohylomys hainanensis\n",
      "  âœ“ Successfully extracted data\n",
      "\n",
      "  ðŸ’¾ Saved intermediate results to vnredlist_species_details_partial_30.json\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m     failed_urls\u001b[38;5;241m.\u001b[39mappend(species_url)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Wait 1-2 seconds between requests\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Save intermediate results every 10 species\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_species_details = []\n",
    "failed_urls = []\n",
    "\n",
    "if not df_species.empty:\n",
    "    total_species = len(df_species)\n",
    "    print(f\"Starting to process {total_species} species...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for idx, row in df_species.iterrows():\n",
    "        species_url = row['url']\n",
    "        scientific_name = row.get('scientific_name', 'Unknown')\n",
    "        \n",
    "        # Progress indicator\n",
    "        if (idx + 1) % 10 == 0 or idx == 0:\n",
    "            print(f\"\\n[{idx + 1}/{total_species}] Processing: {scientific_name}\")\n",
    "        \n",
    "        # Fetch the species page\n",
    "        soup = fetch_species_page(species_url)\n",
    "        \n",
    "        if soup:\n",
    "            try:\n",
    "                # Extract species details\n",
    "                species_data = extract_species_details(species_url, soup)\n",
    "                all_species_details.append(species_data)\n",
    "                \n",
    "                if (idx + 1) % 10 == 0:\n",
    "                    print(f\"  âœ“ Successfully extracted data\")\n",
    "            except Exception as e:\n",
    "                print(f\"  âœ— Error extracting data from {species_url}: {e}\")\n",
    "                failed_urls.append(species_url)\n",
    "        else:\n",
    "            print(f\"  âœ— Failed to fetch: {species_url}\")\n",
    "            failed_urls.append(species_url)\n",
    "        \n",
    "        # Wait 1-2 seconds between requests\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Save intermediate results every 10 species\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            intermediate_file = f'vnredlist_species_details_partial_{idx + 1}.json'\n",
    "            with open(intermediate_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(all_species_details, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"\\n  ðŸ’¾ Saved intermediate results to {intermediate_file}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"âœ“ Processing complete!\")\n",
    "    print(f\"  Successfully extracted: {len(all_species_details)} species\")\n",
    "    print(f\"  Failed: {len(failed_urls)} species\")\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    print(\"No species data to process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18157e7d",
   "metadata": {},
   "source": [
    "## 7. Save Results to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dbc035",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_species_details:\n",
    "    # Save to JSON file\n",
    "    with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_species_details, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"âœ“ All species details saved to: {OUTPUT_JSON}\")\n",
    "    print(f\"  Total records: {len(all_species_details)}\")\n",
    "    \n",
    "    # Save failed URLs for retry\n",
    "    if failed_urls:\n",
    "        failed_file = 'failed_species_urls.txt'\n",
    "        with open(failed_file, 'w') as f:\n",
    "            f.write('\\n'.join(failed_urls))\n",
    "        print(f\"\\nâœ“ Failed URLs saved to: {failed_file}\")\n",
    "        print(f\"  Total failed: {len(failed_urls)}\")\n",
    "else:\n",
    "    print(\"No data to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b11fd31",
   "metadata": {},
   "source": [
    "## 8. Display Sample Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c10e72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_species_details:\n",
    "    print(\"\\nSample extracted species data:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Show first 3 species\n",
    "    for i, species in enumerate(all_species_details[:3], 1):\n",
    "        print(f\"\\n{i}. {species['scientific_name']['value']}\")\n",
    "        print(f\"   Common name: {species['common_name']['value']}\")\n",
    "        print(f\"   Kingdom: {species['kingdom_latin']}\")\n",
    "        print(f\"   Phylum: {species['phylum_latin']}\")\n",
    "        print(f\"   Class: {species['class_latin']}\")\n",
    "        print(f\"   Order: {species['order_latin']}\")\n",
    "        print(f\"   Family: {species['family_latin']}\")\n",
    "        if species['laws']:\n",
    "            print(f\"   Conservation Status: {species['laws'][0]['value']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf13978b",
   "metadata": {},
   "source": [
    "## 9. Statistics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29df6ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_species_details:\n",
    "    print(\"\\nExtraction Statistics:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Count species with complete taxonomic information\n",
    "    complete_taxonomy = sum(1 for s in all_species_details \n",
    "                           if s['kingdom_latin'] and s['phylum_latin'] \n",
    "                           and s['class_latin'] and s['order_latin'] \n",
    "                           and s['family_latin'])\n",
    "    \n",
    "    # Count species with conservation status\n",
    "    with_status = sum(1 for s in all_species_details if s['laws'])\n",
    "    \n",
    "    # Count species with common names\n",
    "    with_common_name = sum(1 for s in all_species_details \n",
    "                          if s['common_name']['value'])\n",
    "    \n",
    "    print(f\"Total species extracted: {len(all_species_details)}\")\n",
    "    print(f\"Species with complete taxonomy: {complete_taxonomy} ({complete_taxonomy/len(all_species_details)*100:.1f}%)\")\n",
    "    print(f\"Species with conservation status: {with_status} ({with_status/len(all_species_details)*100:.1f}%)\")\n",
    "    print(f\"Species with common names: {with_common_name} ({with_common_name/len(all_species_details)*100:.1f}%)\")\n",
    "    print(f\"Failed extractions: {len(failed_urls)}\")\n",
    "    print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "havepawsai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
